\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, amsthm} 
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{listings} 

\geometry{margin=2.5cm}

\lstset{
	language=Python,
	basicstyle=\ttfamily\footnotesize,
	keywordstyle=\color{blue},
	commentstyle=\color{green!40!black},
	stringstyle=\color{purple},
	showstringspaces=false,
	breaklines=true,
	frame=single,
	numbers=left,
	numberstyle=\tiny\color{gray}
}

\title{Sistem de recomandare folosind factorizare de matrice și similaritate pe taguri}
\author{Gatej Stefan-Alexandru, Potop Horia-Ioan}
\date{\today}

\begin{document}
	
	\maketitle
	\tableofcontents
	\newpage
	
	\section{Descrierea aplicației}
	
	Scopul acestei componente a sistemului de recomandare este de a genera recomandări de melodii similare pe baza caracteristicilor extrase din tagurile asociate fiecărei piese muzicale. Problema centrală abordată este factorizarea unei matrice binare (melodie $\times$ tag) într-un spațiu latent de dimensiune redusă, sub constrângeri de non-negativitate. Această factorizare permite descoperirea unor structuri și similarități ascunse între melodii.
	
	\section{Formularea matematică a problemei de factorizare}
	
	Problema de bază pentru metodele de factorizare de matrice (NMF, ALS-NNLS) este următoarea:
	Fie $V \in \mathbb{R}^{m \times n}$ matricea de intrare (în cazul nostru, matricea binară melodie $\times$ tag), unde $m$ este numărul de melodii și $n$ este numărul de taguri unice. Dorim să găsim două matrice, $W \in \mathbb{R}^{m \times r}$ (matricea factorilor latenti pentru melodii) și $H \in \mathbb{R}^{r \times n}$ (matricea factorilor latenti pentru taguri), unde $r \ll \min(m,n)$ este dimensiunea spațiului latent (numărul de factori).
	
	Problema de optimizare este de a minimiza eroarea de reconstrucție, adesea măsurată prin norma Frobenius:
	
	\begin{equation}
		\min_{W, H} \| V - W H \|_F^2
		\label{eq:nmf_objective_main} % Am schimbat label-ul pentru unicitate
	\end{equation}
	
	\textbf{Constrângeri de Non-Negativitate:}
	Pentru a asigura interpretabilitatea factorilor și pentru a se alinia cu natura aditivă a multor date (cum ar fi prezența tagurilor), se impun constrângeri de non-negativitate:
	\begin{itemize}
		\item $W_{ij} \geq 0$ pentru toți $i,j$
		\item $H_{ij} \geq 0$ pentru toți $i,j$
	\end{itemize}
	
	\subsection{Alegerea numărului de factori latenti ($r$)}
	\label{subsec:alegerea_r}
	
	Numărul de factori latenti, $r$, este un hiperparametru crucial în algoritmii de factorizare de matrice, deoarece determină dimensionalitatea spațiului latent în care datele sunt proiectate. O valoare prea mică pentru $r$ poate duce la un model sub-antrenat (underfitting) care nu capturează suficientă informație din datele originale, rezultând într-o eroare de reconstrucție mare și recomandări de calitate inferioară. Pe de altă parte, o valoare prea mare pentru $r$ poate duce la supra-antrenare (overfitting), unde modelul începe să memoreze zgomotul specific setului de date de antrenament și nu generalizează bine pe date noi. De asemenea, un $r$ mare crește complexitatea computațională a antrenării și a utilizării modelului.
	
	În cadrul acestui proiect, pentru a permite o comparație directă și consistentă între diferitele metode de factorizare implementate (NMF Multiplicativ, NMF \texttt{sklearn}, ALS-NNLS), am optat pentru o valoare fixă $r = 10$. Această valoare a fost aleasă ca un punct de pornire rezonabil pentru a demonstra funcționalitatea algoritmilor pe setul de date disponibil, permițând în același timp timpi de execuție gestionabili, în special pentru implementările manuale care sunt mai intensive computațional.
	
	Într-o aplicație practică sau într-un studiu mai aprofundat, selectarea optimă a lui $r$ ar necesita o abordare mai sistematică, implicând experimentarea cu o gamă de valori pentru $r$ și evaluarea performanței modelului folosind metrici adecvate. Metodele comune includ:
	\begin{itemize}
		\item \textbf{Monitorizarea erorii de reconstrucție:} Se analizează cum evoluează eroarea de reconstrucție (de ex., norma Frobenius a diferenței $V - WH$) pe măsură ce $r$ crește. Se caută adesea un punct de inflexiune ("cot" sau "elbow point") în graficul erorii vs. $r$, dincolo de care adăugarea de noi factori nu mai aduce o reducere semnificativă a erorii.
		\item \textbf{Metrici de evaluare specifice sarcinii:} Dacă scopul final este recomandarea și sunt disponibile date de feedback ale utilizatorilor (ex: ratinguri, istoric de ascultare), se pot utiliza metrici de ranking precum Precision@k, Recall@k, NDCG (Normalized Discounted Cumulative Gain) sau MAP (Mean Average Precision) pe un set de date de validare sau test. Valoarea lui $r$ care maximizează aceste metrici ar fi preferată.
		\item \textbf{Stabilitatea factorizării:} Se poate evalua cât de stabile sunt factorizările obținute pentru aceeași valoare $r$ la rulări multiple cu inițializări aleatorii diferite. O stabilitate mai mare este de dorit.
		\item \textbf{Interpretabilitatea factorilor:} În unele aplicații, se preferă un $r$ care produce factori latenti (coloanele lui $W$ sau liniile lui $H$) care au o interpretare semantică clară și relevantă pentru experții în domeniu.
	\end{itemize}
	Pentru scopurile acestei lucrări, ne-am concentrat pe compararea fundamentală a algoritmilor de optimizare cu constrângeri pentru un $r$ fix, recunoscând că optimizarea detaliată a acestui hiperparametru este un pas important, dar distinct, într-un scenariu de dezvoltare a unui sistem de recomandare de producție.
	
	\subsection{Rolul matricelor $W$ și $H$ în sistemul de recomandare}
	În urma procesului de factorizare, obținem două matrice, $W$ și $H$, care au interpretări și utilizări distincte în contextul recomandărilor:
	
	\begin{itemize}
		\item \textbf{Matricea $W \in \mathbb{R}^{m \times r}$ (Melodii $\times$ Factori Latenți):} Fiecare linie $W_i$ din această matrice reprezintă \textbf{profilul latent} al melodiei $i$. Acest vector $r$-dimensional încorporează caracteristicile "ascunse" ale melodiei, așa cum au fost ele învățate de algoritm. Pentru recomandările de tip melodie-melodie, așa cum sunt implementate în acest proiect, similaritatea dintre aceste profiluri latente (de exemplu, similaritatea cosinus între $W_i$ și $W_j$) este utilizată pentru a identifica melodii similare. Astfel, $W$ este matricea principală utilizată direct în funcția de generare a recomandărilor.
		
		\item \textbf{Matricea $H \in \mathbb{R}^{r \times n}$ (Factori Latenți $\times$ Taguri):} Această matrice leagă factorii latenti de tagurile originale. Fiecare coloană $H_j$ poate fi interpretată ca profilul latent al tagului $j$, iar fiecare linie $H_k$ arată importanța sau "încărcătura" factorului latent $k$ pentru fiecare tag original. Matricea $H$ este crucială pentru:
		\begin{itemize}
			\item \textbf{Interpretarea factorilor latenți:} Analizând liniile lui $H$, putem încerca să dăm un sens semantic fiecărui factor latent (ex: un factor ar putea fi puternic asociat cu taguri de "rock clasic", altul cu "muzică electronică ambientală").
			\item \textbf{Recomandarea de taguri pentru melodii:} Produsul $W_i H$ poate fi folosit pentru a prezice relevanța tagurilor pentru melodia $i$.
		\item \textbf{Găsirea de taguri similare:} Similaritatea dintre coloanele lui $H$ poate identifica taguri care sunt conceptual apropiate în spațiul latent.
	\item \textbf{Calitatea factorizării:} Ambele matrice, $W$ și $H$, sunt esențiale pentru reconstrucția $V \approx WH$ și, implicit, pentru calitatea generală a modelului.
	\end{itemize}
\end{itemize}
Deși funcția noastră principală de recomandare utilizează direct matricea $W$, matricea $H$ este o componentă indispensabilă a procesului de învățare și oferă posibilități valoroase de analiză și extindere a sistemului.

\subsection*{Concepte cheie: Profil latent și Cluster latent}

\paragraph{Profil latent}
Un profil latent este o reprezentare numerică (un vector) a unei entități (de exemplu, o melodie) în spațiul de factori ascunși de dimensiune $r$, descoperit de algoritmul de factorizare. Liniile matricei $W$ (adică $W_i$) reprezintă aceste profiluri latente pentru melodii. Acești factori nu sunt direct observați în datele inițiale, ci sunt învățați pentru a surprinde structuri subiacente. De exemplu, un factor ar putea corespunde unui "grad de energie" al melodiei, altul unui "stil vocal specific", etc., chiar dacă aceste concepte nu sunt taguri explicite.

\paragraph{Cluster latent}
Un cluster latent se referă la un grup de entități (melodii) care au profiluri latente similare, adică sunt apropiate în spațiul latent $r$-dimensional. Deși nu aplicăm explicit un algoritm de clustering separat după NMF/ALS în această componentă pentru a defini clustere discrete, ideea este că melodiile cu vectori $W_i$ similari sunt considerate conceptual parte dintr-un "cluster" sau o regiune densă în spațiul latent. Recomandările se bazează pe această proximitate.

\vspace{0.5em}
\textit{Pe scurt: Profilul latent este vectorul de factori ascunși care descrie o melodie, iar similaritatea dintre aceste profiluri stă la baza recomandărilor bazate pe factorizare.}
\vspace{1em} 

\section{Soluții implementate pentru factorizarea cu constrângeri}

Am implementat și comparat trei algoritmi pentru rezolvarea problemei de factorizare de matrice cu constrângeri de non-negativitate:

\subsection{Algoritmul multiplicativ pentru NMF (NMF-MULT)}
Acesta este un algoritm clasic pentru NMF, propus de Lee și Seung. El actualizează iterativ matricele $W$ și $H$ folosind următoarele reguli multiplicative, care garantează non-creșterea funcției obiectiv (\ref{eq:nmf_objective_main}) și menținerea non-negativității:
\begin{align}
H_{kj} &\leftarrow H_{kj} \frac{(W^T V)_{kj}}{(W^T W H + \epsilon)_{kj}} \\
W_{ik} &\leftarrow W_{ik} \frac{(V H^T)_{ik}}{(W H H^T + \epsilon)_{ik}}
\end{align}
unde $\epsilon$ este o constantă mică pentru a evita diviziunea cu zero.
\begin{itemize}
\item \textbf{Avantaje:} Simplu de implementat, relativ rapid pentru multe seturi de date.
\item \textbf{Dezavantaje:} Poate converge lent și la minime locale.
\end{itemize}

\subsection{NMF din scikit-learn (\texttt{sklearn.decomposition.NMF})}
Am utilizat implementarea NMF din biblioteca \texttt{scikit-learn}, care oferă mai mulți solveri. Am ales solver-ul 'cd' (Coordinate Descent), care este eficient și adesea converge mai rapid decât regulile multiplicative simple pentru anumite tipuri de date, minimizând aceeași funcție obiectiv (\ref{eq:nmf_objective_main}) sub constrângeri de non-negativitate.
\begin{itemize}
\item \textbf{Avantaje:} Implementare optimizată, robustă, cu opțiuni pentru regularizare.
\item \textbf{Solver:} Coordinate Descent actualizează fiecare element dintr-o matrice (ținând cealaltă fixă) rezolvând o subproblemă de optimizare univariată.
\end{itemize}

\subsection{Alternating Least Squares cu Non-Negative Least Squares (ALS-NNLS)}
ALS este o altă abordare populară pentru factorizarea de matrice. Algoritmul optimizează alternativ pentru $W$ (ținând $H$ fix) și apoi pentru $H$ (ținând $W$ fix), minimizând funcția obiectiv (\ref{eq:nmf_objective_main}) la fiecare pas. Fiecare subproblemă devine o problemă de least squares. Pentru a impune constrângerile de non-negativitate, fiecare subproblemă de least squares este rezolvată ca o problemă de Non-Negative Least Squares (NNLS).
\begin{itemize}
\item \textbf{Subproblema pentru $W$ (pentru fiecare linie $W_i$):}
$$ \min_{W_i \ge 0} \| V_i - W_i H \|_F^2 $$
\item \textbf{Subproblema pentru $H$ (pentru fiecare coloană $H_j$):} % Corectat: H_j este coloană
$$ \min_{H_{\cdot j} \ge 0} \| V_{\cdot j} - W H_{\cdot j} \|_F^2 $$ 
% Notație: H_.j pentru coloana j a lui H, V_.j pentru coloana j a lui V
\end{itemize}
Am implementat manual atât logica ALS, cât și un solver NNLS bazat pe metoda proiectării gradientului.
\paragraph{NNLS Manual (Proiectarea Gradientului):}
Pentru a rezolva $\min_{x \ge 0} \|Ax - b\|_2^2$:
\begin{enumerate}
\item Se inițializează $x \ge 0$.
\item Se calculează gradientul funcției obiectiv $\frac{1}{2}\|Ax - b\|_2^2$: $\nabla_x = A^T (Ax - b)$.
\item Se face un pas în direcția opusă gradientului: $x \leftarrow x - \eta \nabla_x$.
\item Se proiectează $x$ înapoi pe domeniul fezabil: $x_j \leftarrow \max(0, x_j)$.
\item Se repetă pașii 2-4 până la convergență.
\end{enumerate}
\begin{itemize}
\item \textbf{Avantaje:} Demonstrează înțelegerea detaliată a optimizării cu constrângeri și a descompunerii problemei.
\item \textbf{Dezavantaje:} Implementarea manuală (mai ales NNLS) poate fi lentă și sensibilă la parametrii precum rata de învățare.
\end{itemize}

\section{Rezultate numerice și concluzii}

\subsection{Erori de reconstrucție}
Pentru un număr de factori latenti $r=10$, erorile de reconstrucție (norma Frobenius $\|V - WH\|_F$) obținute au fost:
\begin{itemize}
\item \textbf{NMF Multiplicativ (Manual):} 337.03
\item \textbf{NMF (sklearn, solver 'cd'):} 336.42
\item \textbf{ALS (NNLS Manual):} 430.48
\end{itemize}
Se observă că implementarea din \texttt{sklearn} obține o eroare de reconstrucție puțin mai mică, probabil datorită optimizărilor interne și a robusteții solver-ului Coordinate Descent. Algoritmul multiplicativ obține rezultate comparabile. Implementarea ALS-NNLS manuală prezintă o eroare semnificativ mai mare, indicând posibile probleme de convergență în subproblemele NNLS manuale, în special legate de alegerea ratei de învățare și a numărului de iterații pentru solver-ul de proiectare a gradientului.

\subsection{Recomandări generate (Exemplu)}
\label{subsec:recomandari_exemplu}

Pentru a ilustra tipurile de recomandări generate de fiecare metodă, am selectat piesa de referință \textbf{"Murder One" de Metallica}. Această piesă este un tribut adus lui Lemmy Kilmister de la Motörhead și se încadrează în genul Thrash Metal/Heavy Metal. Mai jos sunt prezentate top 5 recomandări pentru fiecare abordare implementată:

\begin{itemize}
	\item \textbf{Referință: Murder One - Metallica}
	\begin{itemize}
		\item \textit{Recomandări (Tag Similarity - Similaritate Cosinus pe taguri):}
		\begin{itemize}
			\item Necrophiliac - Slayer
			\item Blacklist - Exodus
			\item Electric Crown - Testament
			\item Murder Fantasies - Kreator
			\item Practice What You Preach - Testament
		\end{itemize}
		\vspace{0.5em} % Spațiu mic între metode
		\item \textit{Recomandări (NMF Multiplicativ Manual):}
		\begin{itemize}
			\item Crionics - Slayer
			\item Praise of Death - Slayer
			\item Blackmail The Universe - Megadeth
			\item Washington Is Next! - Megadeth
			\item Back In the Day - Megadeth
		\end{itemize}
		\vspace{0.5em}
		\item \textit{Recomandări (NMF \texttt{sklearn}, solver 'cd'):}
		\begin{itemize}
			\item Blacklist - Exodus
			\item At Dawn They Sleep - Slayer
			\item Bite The Hand - Megadeth
			\item Uplift - Pantera
			\item Praise of Death - Slayer
		\end{itemize}
		\vspace{0.5em}
		\item \textit{Recomandări (ALS cu NNLS Manual):}
		\begin{itemize}
			\item Practice What You Preach - Testament
			\item Bite The Hand - Megadeth
			\item Hy Pro Glo - Anthrax
			\item Deviance - Slayer
			\item Tormentor - Slayer
		\end{itemize}
	\end{itemize}
\end{itemize}

\paragraph{Observații asupra exemplului:}
Se poate observa că toate metodele tind să recomande piese din subgenuri similare ale metalului (Thrash, Speed, Heavy Metal), ceea ce este de așteptat.
\begin{itemize}
	\item \textbf{Similaritatea pe taguri} pare să identifice corect piese cu tematici sau stiluri lirice/muzicale apropiate, menționate explicit în taguri (ex: Kreator - Murder Fantasies).
	\item \textbf{Metodele NMF} (atât manuală, cât și \texttt{sklearn}) și \textbf{ALS} generează recomandări care, deși tot din sfera metal, pot proveni de la artiști diferiți, indicând descoperirea unor profiluri latente comune. De exemplu, prezența consistentă a pieselor de la Slayer și Megadeth în recomandările NMF și ALS sugerează că aceste trupe partajează caracteristici latente similare cu Metallica în contextul datelor analizate.
	\item Diferențele subtile între recomandările NMF și ALS pot fi atribuite modului diferit în care algoritmii converg și ponderează factorii latenti, precum și performanței de reconstrucție a fiecărei metode.
\end{itemize}
\paragraph{}{Acest exemplu ilustrează capacitatea metodelor de factorizare de a merge dincolo de potrivirile directe de taguri și de a sugera melodii relevante pe baza unor similarități structurale mai profunde.}
\paragraph{}{Recomandările bazate pe factorizare (NMF, ALS) tind să identifice similarități latente. Calitatea recomandărilor ALS-NNLS este direct influențată de performanța sa în minimizarea erorii de reconstrucție.}

\subsection{Analiza convergenței și a erorilor de reconstrucție}
\label{subsec:convergenta-erori} % Am schimbat _ cu - în label

Performanța algoritmilor de factorizare este adesea evaluată prin eroarea de reconstrucție și prin modul în care aceasta evoluează pe parcursul iterațiilor.

\paragraph{NMF Multiplicativ (Manual):}
După cum se observă din log-ul de execuție, algoritmul NMF multiplicativ prezintă o scădere monotonă a erorii de reconstrucție pe măsură ce numărul de iterații crește:
\begin{verbatim}
	Multiplicative NMF Iteration 0: error = 403.9834
	Multiplicative NMF Iteration 10: error = 350.3428
	...
	Multiplicative NMF Iteration 90: error = 337.0462
\end{verbatim}
Eroarea finală raportată a fost de \textbf{337.03}. Această convergență graduală este caracteristică regulilor de update multiplicative.

\paragraph{NMF (\texttt{sklearn}, solver 'cd'):}
Implementarea din \texttt{sklearn} cu solver-ul Coordinate Descent a raportat direct o eroare finală de reconstrucție de \textbf{336.42}. Acest solver este adesea mai rapid în a atinge un minim local bun datorită optimizărilor sale.

\paragraph{ALS cu NNLS Manual (ALS-NNLS):}
Pentru implementarea ALS cu subprobleme NNLS rezolvate manual, s-a observat următorul comportament:
\begin{verbatim}
	ALS-NNLS Iteration 0: Frobenius error = 426.8031
	ALS-NNLS Iteration 5: Frobenius error = 430.4727
	ALS-NNLS converged at iteration 8.
\end{verbatim}
Eroarea finală de reconstrucție raportată a fost de aproximativ \textbf{430.48}. Se observă că eroarea inițială este mai mare comparativ cu NMF și, în mod interesant, eroarea a crescut ușor între iterația 0 și iterația 5, înainte de a se stabiliza și a algoritmului să conveargă la iterația 8. Această creștere temporară a erorii poate apărea în ALS dacă subproblemele NNLS nu sunt rezolvate cu suficientă precizie la fiecare pas, sau dacă rata de învățare aleasă pentru NNLS manual (\texttt{lr\_nnls = 1e-4}) și numărul de iterații interne NNLS (\texttt{nnls\_iters = 50}) nu sunt optimi pentru toate subproblemele întâlnite.

Este important de menționat că parametrii pentru ALS-NNLS manual (precum \texttt{als\_iters = 30}, \texttt{nnls\_iters = 50}, \texttt{lr\_nnls = 1e-4}) au fost aleși pentru a obține un echilibru între calitatea soluției și timpul de execuție, în special având în vedere natura computațional intensivă a rezolvării manuale a multiplelor subprobleme NNLS. O explorare mai exhaustivă a spațiului de hiperparametri pentru \texttt{lr\_nnls} și \texttt{nnls\_iters} ar putea duce la o eroare de reconstrucție mai mică pentru ALS-NNLS, dar ar crește semnificativ timpul de rulare. Valoarea de "Explained variance" raportată pentru ALS-NNLS este foarte mică, ceea ce este consistent cu o eroare de reconstrucție mare și sugerează că modelul, în configurația actuală, nu a reușit să captureze la fel de bine structura datelor ca metodele NMF.

\subsection{Erori de reconstrucție (Sumar)} 
Pentru un număr de factori latenti $r=10$, erorile de reconstrucție (norma Frobenius $\|V - WH\|_F$) obținute au fost:
\begin{itemize}
	\item \textbf{NMF Multiplicativ (Manual):} 337.03
	\item \textbf{NMF (\texttt{sklearn}, solver 'cd'):} 336.42
	\item \textbf{ALS (NNLS Manual):} 430.48
\end{itemize}

% ... (secțiunea \subsection{Recomandări generate (Exemplu)} rămâne la fel, presupunând că numele pieselor nu conțin underscore problematic) ...
% Dacă numele pieselor conțin underscore, trebuie escapate: Metallica\_MasterOfPuppets -> Metallica\textunderscore MasterOfPuppets

\subsection{Timp de execuție și Complexitate Computațională} 
\begin{itemize}
	\item \textbf{NMF (\texttt{sklearn}):} Cel mai rapid, datorită implementării optimizate în C și a eficienței algoritmului Coordinate Descent.
	\item \textbf{NMF Multiplicativ (Manual):} Relativ rapid. Fiecare iterație implică operații matriciale, dar nu rezolvarea unor subprobleme de optimizare distincte.
	\item \textbf{ALS (NNLS Manual):} Cel mai lent. Fiecare iterație a buclei principale ALS necesită rezolvarea a $m+n$ subprobleme NNLS. Deoarece fiecare subproblemă NNLS este rezolvată printr-un algoritm iterativ (proiectarea gradientului), costul computațional total este semnificativ mai mare. Timpul de execuție este direct influențat de \texttt{max\_iter\_als}, \texttt{max\_iter\_nnls} și de eficiența implementării NNLS. Parametrii aleși în experimentele noastre au vizat un compromis pentru a permite finalizarea într-un timp rezonabil.
\end{itemize}

\subsection{Concluzii specifice metodelor}
\begin{itemize}
	\item Metodele NMF (atât implementarea manuală multiplicativă, cât și cea din \texttt{sklearn}) au demonstrat capacitatea de a reduce eroarea de reconstrucție la valori comparabile, \texttt{sklearn} fiind marginal superior și mai rapid.
	\item Implementarea ALS cu NNLS manual a prezentat o eroare de reconstrucție considerabil mai mare în configurația testată. Acest rezultat subliniază provocările asociate cu implementarea manuală a algoritmilor de optimizare iterativi imbricați, în special sensibilitatea la hiperparametri precum rata de învățare și numărul de iterații pentru subproblemele NNLS. Deși ALS este o metodă puternică, performanța sa aici a fost limitată de eficiența și convergența solver-ului NNLS manual. Alegerea parametrilor a fost un compromis pentru a asigura finalizarea execuției într-un interval de timp acceptabil pentru testare.
	\item Toate metodele de factorizare generează profiluri latente care pot fi utilizate pentru recomandări, dar calitatea acestora (și, implicit, a recomandărilor) este corelată cu capacitatea metodei de a minimiza eroarea de reconstrucție.
\end{itemize}

\section{Comparare cu alte funcții Python}
% ... (rămâne la fel) ...
Pentru problema de factorizare non-negativă, funcțiile generice de optimizare cu constrângeri din \texttt{scipy.optimize.minimize} (ex: cu solver-ul 'SLSQP' sau 'L-BFGS-B' cu bounds) ar putea fi teoretic aplicate. Totuși, acestea ar necesita "aplatizarea" matricelor $W$ și $H$ într-un singur vector de variabile, ceea ce ar duce la un număr foarte mare de variabile ($m \times r + r \times n$). Pentru astfel de probleme la scară mare cu structură specifică (factorizare de matrice), algoritmii dedicați precum NMF multiplicativ, Coordinate Descent (din \texttt{sklearn}), sau ALS sunt semnificativ mai eficienți. Am ales să implementăm ALS cu NNLS manual ca o altă metodă specifică problemei, dar care ilustrează optimizarea iterativă a subproblemelor cu constrângeri.

\section{Bibliografie}
% ... (rămâne la fel, poate adaugi o referință specifică pentru ALS dacă găsești una bună) ...
\begin{itemize}
\item Lee, D. D., \& Seung, H. S. (2001). Algorithms for non-negative matrix factorization. \textit{Advances in neural information processing systems, 13}.
\item Chih-Jen Lin. (2007). Projected Gradient Methods for Non-negative Matrix Factorization. \textit{Neural Computation, 19(10)}, 2756-2779. (Referință pentru metode de gradient pentru NMF/NNLS)
\item Zdunek, R., \& Cichocki, A. (2007). Non-negative matrix factorization with quadratic programming. \textit{Computers \& Mathematics with Applications, 54(1)}, 70-83. (Discută despre NNLS în context NMF)
\item \url{https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html}
\item \url{https://en.wikipedia.org/wiki/Non-negative_matrix_factorization}
\item \url{https://en.wikipedia.org/wiki/Alternating_least_squares}
\end{itemize}

\end{document}
